{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Image Deraining using Convolutional Neural Networks\n","\n","<img src=\"https://raw.githubusercontent.com/andywang947/Common/main/9.PNG\" width=\"500px\">"],"metadata":{"id":"06_rpoG514bn"}},{"cell_type":"markdown","source":["**Image credit: Rain100L Dataset**  \n","\n","After this homework, you would ideally have learned:  \n","\n","- To create a simple solution that is easy to set up.\n","- Learning about the principles of image denoising.\n","- To manage the image data and apply augmentation techniques to the images.\n","- To train and optimize the model for better performance.\n","- To develop strategies that can help navigate through various options to find the best solution."],"metadata":{"id":"gKJMGiEc02Q5"}},{"cell_type":"markdown","source":["## Introduction\n","\n","Low-level tasks in computer vision, like color enhancement, denoising, dehazing, are a very important and fundamental step in computer vision. Correcting a degraded image through image processing methods is crucial for subsequent computer vision tasks. For example, in **Assignment 3**, even the results obtained during training were excellent, applying it in reality scenarios with degradation images would significantly decrease the performance.\n","\n","In this assignment, you will attempt to use a CNN for the task of rain removal, which we ofter call it **deraining** task. The presence of rain in an image poses a challenge to machine vision, similar to the difficulty faced when driving with a broken windshield wiper."],"metadata":{"id":"x7SOGg2LvbRB"}},{"cell_type":"markdown","source":["## Homework\n","\n","In this assignment, we will train a CNN-based image deraining network using a dataset called Rain100L, which contains 200 training images and 100 testing images. The goal of this assignment is to train an image deraining network using CNN and to improve the model through various experiments. Please follow the instructions below to build the image deraining network."],"metadata":{"id":"L-03qwjRvkG4"}},{"cell_type":"markdown","source":["## Get the dataset\n","\n","Like the previous work, we need to connect with your google drive to get data for this training.\n","\n","**First, create share folder's shortcut to your google drive.**  \n","1. Go google drive website and login  \n","2. Go to https://drive.google.com/drive/folders/1D1A_Rx5LSDxW0VSswwM1V1bGvibOho8_?usp=sharing\n","3. Create shared folder's shortcut on your dirve  \n","<img src=\"https://github.com/andywang947/Common/blob/main/10.PNG?raw=true\" width=\"800px\">  \n","<img src=\"https://github.com/andywang947/Common/blob/main/11.PNG?raw=true\" width=\"800px\">  \n","\n","4. You'll see the folder is on your drive now!  \n","<img src=\"https://github.com/andywang947/Common/blob/main/12.PNG?raw=true\" width=\"800px\">  \n","\n","5. Done!"],"metadata":{"id":"-MuZUEsjvl1j"}},{"cell_type":"markdown","source":["Next, let's check if you have correctly obtained the files.\n","\n","(optional: We recommend you use T4 GPU to do the training. You can check the assignment 3 for the detail.)\n","\n","First, connect to your google drive."],"metadata":{"id":"EzCMZW1pARNu"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"TuL1PxOTAeSR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","file_path = \"/content/drive/MyDrive/HW4_revised/HW4_image_deraining.zip\"\n","if os.path.exists(file_path):\n","  print(\"You got the file !\")\n","else :\n","  print(\"You didn't obtain the files now, need to correct your path.\")"],"metadata":{"id":"fn1hblq1ArsZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you check you got the file correctly, next, we can unzip the files."],"metadata":{"id":"oWG2O0C3BBf7"}},{"cell_type":"code","source":["! unzip /content/drive/MyDrive/HW4_revised/HW4_image_deraining.zip -d /content/"],"metadata":{"id":"l_Is_YpuBHON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","def check_data(data_path) :\n","  if os.path.exists(data_path) :\n","    files = os.listdir(data_path)\n","    print(\"There are \",len(files),\"images in the dictionary:\",data_path)\n","  else :\n","    print(\"The dictionary path is wrong ! You need to fix your dictionary path !\")\n","\n","train_input_dir = \"/content/Rain100L/train/input\"\n","check_data(train_input_dir)\n","train_target_dir = \"/content/Rain100L/train/target\"\n","check_data(train_target_dir)\n","test_input_dir = \"/content/Rain100L/test/input\"\n","check_data(test_input_dir)\n","test_target_dir = \"/content/Rain100L/test/target\"\n","check_data(test_target_dir)"],"metadata":{"id":"_3GRu-MnBP0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You should see you have 200,200,100,100 images in the dictionaries."],"metadata":{"id":"fiqsRSPFBSw-"}},{"cell_type":"markdown","source":["## Dataloader"],"metadata":{"id":"3c4yhbz8vzyv"}},{"cell_type":"markdown","source":["Now, we have all data correctly. Next, we can start our training process.\n","\n","First, we need to load our data."],"metadata":{"id":"2qDb6RiMBe-f"}},{"cell_type":"code","source":["import os\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","class PairedImageDataset(Dataset):\n","    def __init__(self, input_dir, target_dir, transform=None):\n","        self.input_filenames = [os.path.join(input_dir, f) for f in sorted(os.listdir(input_dir))]\n","        self.target_filenames = [os.path.join(target_dir, f) for f in sorted(os.listdir(target_dir))]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.input_filenames)\n","\n","    def __getitem__(self, idx):\n","        input_image = Image.open(self.input_filenames[idx]).convert('RGB')\n","        target_image = Image.open(self.target_filenames[idx]).convert('RGB')\n","\n","        if self.transform:\n","            input_image = self.transform(input_image)\n","            target_image = self.transform(target_image)\n","\n","        return input_image, target_image\n","\n","transform = transforms.Compose([\n","    transforms.Resize((112, 112)),\n","    transforms.ToTensor(),\n","])\n","\n","# Dataset\n","train_dataset = PairedImageDataset(train_input_dir, train_target_dir, transform=transform)\n","test_dataset = PairedImageDataset(test_input_dir, test_target_dir, transform=transform)\n","\n","# DataLoader\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n","\n","\n","# Fetch a batch and display the images\n","data_iter = iter(train_dataloader)\n","images, targets = next(data_iter)\n","\n","# Display images using matplotlib\n","fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n","\n","for i in range(4):\n","    img_input = images[i].numpy().transpose((1, 2, 0))\n","    img_input = img_input.clip(0, 1)\n","\n","    img_target = targets[i].numpy().transpose((1, 2, 0))\n","    img_target = img_target.clip(0, 1)\n","\n","    ax_input = axes[0, i]\n","    ax_input.imshow(img_input)\n","    ax_input.axis('off')\n","    ax_input.set_title(f'Input {i+1}')\n","\n","    ax_target = axes[1, i]\n","    ax_target.imshow(img_target)\n","    ax_target.axis('off')\n","    ax_target.set_title(f'Target {i+1}')\n","\n","plt.show()"],"metadata":{"id":"5n1-pA6SJStY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we know we have our dataloader, which can feed the images to our model.\n","And by showing the data, you can see our input data and the target data."],"metadata":{"id":"y-8W-_ALJqJr"}},{"cell_type":"markdown","source":["## Model\n","\n","Next, let's build our CNN model.\n","Here I use the Deep, narrow, CNN as the model here."],"metadata":{"id":"BaGo3Sn6v2TO"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchsummary import summary\n","\n","\n","class DeepNarrowCNN(nn.Module):\n","    def __init__(self):\n","        super(DeepNarrowCNN, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU()\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 3, kernel_size=3, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","model = DeepNarrowCNN()\n","# if you use GPU, you can speed your training time.\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","model = model.to(device)"],"metadata":{"id":"_PAlAjMYJxkQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(model, (3, 112, 112))"],"metadata":{"id":"PzpDpOqdKsoi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By the summary of our model, we can see the structure of our model, and the number of parameters. These are important reference materials for analyzing the performance and quality of the model.\n","\n","Here, we have 47283 parameters."],"metadata":{"id":"NpcfS4xWKxHu"}},{"cell_type":"markdown","source":["## Training\n","\n","Now, we can start our training.\n","Here, I set the training process to do 100 epochs, if epochs bigger than 50 and loss didn't decrease in 10 epochs, then early stopping the training."],"metadata":{"id":"WHjlCQQhB8Og"}},{"cell_type":"code","source":["import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","\n","def unnormalize(image):\n","    image = image.numpy().transpose((1, 2, 0))\n","    image = np.clip(image, 0, 1)\n","    return image\n","\n","def testing(model,test_dataloader):\n","  model.eval()\n","  model.to('cpu')\n","\n","  data_iter = iter(test_dataloader)\n","  inputs, targets = next(data_iter)\n","\n","  with torch.no_grad():\n","      predictions = model(inputs)\n","\n","  inputs = inputs.cpu()\n","  targets = targets.cpu()\n","  predictions = predictions.cpu()\n","\n","  fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(12, 9))\n","  for i in range(4):\n","      ax = axes[0, i]\n","      ax.imshow(unnormalize(inputs[i]))\n","      ax.axis('off')\n","      ax.set_title('Input Image')\n","\n","      ax = axes[1, i]\n","      ax.imshow(unnormalize(predictions[i]))\n","      ax.axis('off')\n","      ax.set_title('Predicted Image')\n","\n","      ax = axes[2, i]\n","      ax.imshow(unnormalize(targets[i]))\n","      ax.axis('off')\n","      ax.set_title('Target Image')\n","\n","  plt.show()\n","\n","def training(criterion,optimizer,model):\n","\n","  num_epochs = 100\n","  testing_epoch = 10\n","  patience = 10  # For Early stopping use\n","  best_loss = float('inf')\n","  epochs_no_improve = 0\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  print(f'Using device: {device}')\n","  model = model.to(device)\n","\n","  for epoch in range(num_epochs):\n","      running_loss = 0.0\n","      for inputs, targets in train_dataloader:\n","          inputs, targets = inputs.to(device), targets.to(device)\n","          optimizer.zero_grad()\n","          outputs = model(inputs)\n","          loss = criterion(outputs, targets)\n","          loss.backward()\n","          optimizer.step()\n","          running_loss += loss.item()\n","      epoch_loss = running_loss / len(train_dataloader)\n","      print(f'Epoch {epoch+1}, Loss: {epoch_loss}')\n","\n","      if epoch % testing_epoch == 0 :\n","        testing(model,test_dataloader)\n","        model = model.to(device)\n","\n","      if epoch > 50 :\n","        if epoch_loss < best_loss:\n","            best_loss = epoch_loss\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","        if epochs_no_improve >= patience:\n","            print(f'Early stopping at epoch {epoch+1}')\n","            break"],"metadata":{"id":"lXyLjEQvRESv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the first model, here we use the previous DeepNarrowCNN as the baseline model, and L1 loss, and NAdam as the optimizer.\n","Let's do the training to see the results."],"metadata":{"id":"FCxC5iW-XGJv"}},{"cell_type":"code","source":["model = DeepNarrowCNN()\n","criterion = nn.L1Loss()\n","optimizer = optim.NAdam(model.parameters(), lr=0.001)\n","training(criterion,optimizer,model)"],"metadata":{"id":"UQRCw1Qyif5D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation\n","\n","After the training, we can test our model by evaluation.\n","First, let's see our model outputs, if they satisfy our task?"],"metadata":{"id":"T7935qkdCA8w"}},{"cell_type":"code","source":["testing(model,test_dataloader)"],"metadata":{"id":"2B8hvmBVTQuD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It's not accurate to rely solely on our eyes, so here we use **PSNR(Peak signal-to-noise ratio)** and **SSIM(structural similarity)** metrics to evaluate the performance of the rain removal, which are common metrics in low-level vision tasks."],"metadata":{"id":"ix4inG0pTV6T"}},{"cell_type":"code","source":["import torch\n","from skimage.metrics import peak_signal_noise_ratio as psnr\n","from skimage.metrics import structural_similarity as ssim\n","import numpy as np\n","\n","def calculate_psnr_ssim(inputs, predictions, targets):\n","    psnr_values = []\n","    ssim_values = []\n","\n","    inputs_np = inputs.numpy().transpose((0, 2, 3, 1))\n","    predictions_np = predictions.numpy().transpose((0, 2, 3, 1))\n","    targets_np = targets.numpy().transpose((0, 2, 3, 1))\n","\n","    predictions_np = np.clip(predictions_np, 0, 1)\n","    targets_np = np.clip(targets_np, 0, 1)\n","\n","    for i in range(inputs.shape[0]):\n","        psnr_val = psnr(targets_np[i], predictions_np[i], data_range=1)\n","        ssim_val, _ = ssim(targets_np[i], predictions_np[i], data_range=1, channel_axis=-1, full=True)\n","\n","        psnr_values.append(psnr_val)\n","        ssim_values.append(ssim_val)\n","\n","    return np.mean(psnr_values), np.mean(ssim_values)\n","\n","def evaluation(model,test_dataloader) :\n","  model.eval()\n","  model.to('cpu')\n","\n","  total_psnr = 0\n","  total_ssim = 0\n","  count = 0\n","  all_predictions = []\n","\n","  for inputs, targets in test_dataloader:\n","      with torch.no_grad():\n","          predictions = model(inputs)\n","\n","      inputs = inputs.cpu()\n","      predictions = predictions.cpu()\n","      all_predictions.append(predictions)\n","      targets = targets.cpu()\n","\n","      psnr_val, ssim_val = calculate_psnr_ssim(inputs, predictions, targets)\n","      total_psnr += psnr_val\n","      total_ssim += ssim_val\n","      count += 1\n","\n","  all_predictions = np.concatenate(all_predictions, axis=0)\n","  average_psnr = total_psnr / count\n","  average_ssim = total_ssim / count\n","\n","  print(f'Average PSNR: {average_psnr:.2f}')\n","  print(f'Average SSIM: {average_ssim:.2f}')"],"metadata":{"id":"WHnUwayVT_7J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation(model,test_dataloader)"],"metadata":{"id":"G2wETu9SUFoD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ways to improve"],"metadata":{"id":"IwuLZ9kYv3H8"}},{"cell_type":"markdown","source":["Now, we have a baseline model! This is often the first step in the training process, and then we need to try to improve the model for better performance!"],"metadata":{"id":"DmeVWwNLUas9"}},{"cell_type":"markdown","source":["### Optimizer\n","\n","First of all, optimizer, in the previous model, we use NAdam as our optimizer. Now, let's change the optimizer to Adam, which is also a gradient descent way to optimize the parameters."],"metadata":{"id":"TX71shoKvxvr"}},{"cell_type":"code","source":["model = DeepNarrowCNN()\n","criterion = nn.L1Loss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001) # Change the optimizer to Adam\n","training(criterion,optimizer,model)"],"metadata":{"id":"-ta-a0eHbaDa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testing(model,test_dataloader)"],"metadata":{"id":"jfHtv9ygamjY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation(model,test_dataloader)"],"metadata":{"id":"FGMSMquuapjW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Loss function\n","\n","Another way to improve is about the choice of loss function.\n","\n","Above we use L1 loss, which is the absolute distance between the output and the target.\n","\n","Next, let's try MSE (Mean Square Error) as the loss function."],"metadata":{"id":"1TipPgi4vec5"}},{"cell_type":"code","source":["model = DeepNarrowCNN()\n","criterion = nn.MSELoss()  # Use MSE as the loss function.\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","training(criterion,optimizer,model)"],"metadata":{"id":"OVwsOqYcWCTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testing(model,test_dataloader)"],"metadata":{"id":"LdGb1kMIaw0b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation(model,test_dataloader)"],"metadata":{"id":"yCIJXf18ayth"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### The structure of the model\n","\n","Another way to improve is to change the stucture of the model,"],"metadata":{"id":"oYROseDDwB8v"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class WideCNN(nn.Module):\n","    def __init__(self):\n","        super(WideCNN, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU()\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(16, 3, kernel_size=3, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x\n","\n","model = WideCNN() # Our new model, with the classical encoder-decoder structure\n","model = model.to(device)"],"metadata":{"id":"fz-WzYzgbLW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["summary(model, (3, 112, 112))"],"metadata":{"id":"IzwgH9pmYKB9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["By the summary, we can see the model parameters are still about the same as the previous DeepNarrowCNN, but the strucutre design is completly different. Next, let's train the model and see the results."],"metadata":{"id":"TT-xnnyKYTHh"}},{"cell_type":"code","source":["model = WideCNN() # Our new model, with the classical encoder-decoder structure\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","training(criterion,optimizer,model)"],"metadata":{"id":"Z9BEunsAfyLE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testing(model,test_dataloader)"],"metadata":{"id":"2YY9plo4f18B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation(model,test_dataloader)"],"metadata":{"id":"ZbxH7Pw6f3M1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" After using this classic encoder-decoder architecture, you should see a significant improvement in the results! This proves that in the world of deep learning, simply increasing the complexity of the model does not always lead to better results. Smart design can improve performance while maintaining the number of parameters!"],"metadata":{"id":"1N48dvwEYpmB"}},{"cell_type":"markdown","source":["### Data augmentation\n","\n","Again, after the assignment 3, we see the signigicant improve after data augmentation through we don't have so many images in our dataset.\n","\n","Here, we can also try two different ways to improve."],"metadata":{"id":"5G6ZvwASwMtV"}},{"cell_type":"markdown","source":["We need to add more images to the new traininng dataset."],"metadata":{"id":"HN0gfLloG-XP"}},{"cell_type":"code","source":["import os\n","import random\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torchvision import transforms\n","\n","old_train_dir = \"/content/Rain100L/train\"\n","new_train_dir = \"/content/Rain100L/newtrain\"\n","\n","if not os.path.exists(new_train_dir):\n","    os.makedirs(new_train_dir)\n","    print(f\"Directory '{new_train_dir}' created.\")\n","    os.makedirs(os.path.join(new_train_dir, \"input\"))\n","    os.makedirs(os.path.join(new_train_dir, \"target\"))\n","else:\n","    print(f\"Directory '{new_train_dir}' already exists.\")\n","\n","old_input_dir = os.path.join(old_train_dir, \"input\")\n","old_target_dir = os.path.join(old_train_dir, \"target\")\n","\n","new_input_dir = os.path.join(new_train_dir, \"input\")\n","new_target_dir = os.path.join(new_train_dir, \"target\")\n","\n","input_filenames = [os.path.join(old_input_dir, f) for f in sorted(os.listdir(old_input_dir))]\n","target_filenames = [os.path.join(old_target_dir, f) for f in sorted(os.listdir(old_target_dir))]\n","\n","base_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","])\n","\n","for i, (input_filename, target_filename) in enumerate(zip(input_filenames, target_filenames)):\n","    original_input_image = Image.open(input_filename).convert('RGB')\n","    original_target_image = Image.open(target_filename).convert('RGB')\n","\n","    flipped_input_image = transforms.functional.vflip(original_input_image)\n","    flipped_target_image = transforms.functional.vflip(original_target_image)\n","\n","    angle = random.uniform(-15, 15)\n","    rotated_input_image = transforms.functional.rotate(original_input_image, angle)\n","    rotated_target_image = transforms.functional.rotate(original_target_image, angle)\n","\n","    original_input_image.save(os.path.join(new_input_dir, f\"original_input_{i}.png\"))\n","    original_target_image.save(os.path.join(new_target_dir, f\"original_target_{i}.png\"))\n","\n","    flipped_input_image.save(os.path.join(new_input_dir, f\"flipped_input_{i}.png\"))\n","    flipped_target_image.save(os.path.join(new_target_dir, f\"flipped_target_{i}.png\"))\n","\n","    rotated_input_image.save(os.path.join(new_input_dir, f\"rotated_input_{i}.png\"))\n","    rotated_target_image.save(os.path.join(new_target_dir, f\"rotated_target_{i}.png\"))\n"],"metadata":{"id":"-YtLFb5TLn0D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","def check_data(data_path) :\n","  if os.path.exists(data_path) :\n","    files = os.listdir(data_path)\n","    print(\"There are \",len(files),\"images in the dictionary:\",data_path)\n","  else :\n","    print(\"The dictionary path is wrong ! You need to fix your dictionary path !\")\n","\n","train_input_dir = new_input_dir\n","check_data(train_input_dir)\n","train_target_dir = new_target_dir\n","check_data(train_target_dir)"],"metadata":{"id":"uu83H5G5NMvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","class PairedImageDataset(Dataset):\n","    def __init__(self, input_dir, target_dir, transform=None):\n","        self.input_filenames = [os.path.join(input_dir, f) for f in sorted(os.listdir(input_dir))]\n","        self.target_filenames = [os.path.join(target_dir, f) for f in sorted(os.listdir(target_dir))]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.input_filenames)\n","\n","    def __getitem__(self, idx):\n","        input_image = Image.open(self.input_filenames[idx]).convert('RGB')\n","        target_image = Image.open(self.target_filenames[idx]).convert('RGB')\n","\n","        if self.transform:\n","            input_image = self.transform(input_image)\n","            target_image = self.transform(target_image)\n","\n","        return input_image, target_image\n","\n","transform = transforms.Compose([\n","    transforms.Resize((112, 112)),\n","    transforms.ToTensor(),\n","])\n","\n","# Dataset\n","train_dataset = PairedImageDataset(new_input_dir, new_target_dir, transform=transform)\n","test_dataset = PairedImageDataset(test_input_dir, test_target_dir, transform=transform)\n","\n","# DataLoader\n","train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n","\n","\n","# Fetch a batch and display the images\n","data_iter = iter(train_dataloader)\n","images, targets = next(data_iter)\n","\n","# Display images using matplotlib\n","fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n","\n","for i in range(4):\n","    img_input = images[i].numpy().transpose((1, 2, 0))\n","    img_input = img_input.clip(0, 1)\n","\n","    img_target = targets[i].numpy().transpose((1, 2, 0))\n","    img_target = img_target.clip(0, 1)\n","\n","    ax_input = axes[0, i]\n","    ax_input.imshow(img_input)\n","    ax_input.axis('off')\n","    ax_input.set_title(f'Input {i+1}')\n","\n","    ax_target = axes[1, i]\n","    ax_target.imshow(img_target)\n","    ax_target.axis('off')\n","    ax_target.set_title(f'Target {i+1}')\n","\n","plt.show()"],"metadata":{"id":"Fe-Dwl8QMThZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = WideCNN()\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","training(criterion,optimizer,model)"],"metadata":{"id":"AN3BCdXpNj9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testing(model,test_dataloader)"],"metadata":{"id":"UiJBk9S-P-FD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation(model,test_dataloader)"],"metadata":{"id":"J8Olm0NzP-nK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Add more parameters"],"metadata":{"id":"B5yv9tvKwGI-"}},{"cell_type":"markdown","source":["Besides change the structure of the model, we can still add more layers to the model.\n","\n","But since if we add more parameters to the model, it costs more time to the training process. So through it's an easy way to get better performance, but here we try this improve in the end."],"metadata":{"id":"yvAD2s95gYlg"}},{"cell_type":"code","source":["class DeepWideCNN(nn.Module):\n","    def __init__(self):\n","        super(DeepWideCNN, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU()\n","        )\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.ConvTranspose2d(32, 3, kernel_size=3, padding=1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"],"metadata":{"id":"yG--bce5f4rA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = DeepWideCNN()\n","criterion = nn.MSELoss()  # Use MSE as the loss function.\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","training(criterion,optimizer,model)"],"metadata":{"id":"jzcznviWgrpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testing(model,test_dataloader)"],"metadata":{"id":"e_mUeQdXgvvl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluation(model,test_dataloader)"],"metadata":{"id":"u-6vlFt9gxE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If our environments are the same, you should be able to see that one of the PSNR values in the above settings exceeds 30! This process is a classic deep learning approach to problem-solving. Hope you enjoy this deep learning journey!"],"metadata":{"id":"rj-lSDDKlect"}},{"cell_type":"markdown","source":["## Problems\n","\n","1. The major challenge in addressing the deraining problem, as compared to issues like image classification and facial recognition, is forming paired datasets. It's easy to collect scenes with rain, but obtaining corresponding clean images is extremely difficult. Consequently, many synthetic datasets exist today, such as the rain100L used in this assignment. What impact do you think using synthetic datasets might have on training results, and how can we solve this problem? (Maybe we don't need paired data?) Please provide one idea.\n","\n","  You may find some interesting ideas in the following papers.\n","\n","  Noise2Noise: Learning Image Restoration without Clean Data\n","  https://arxiv.org/abs/1803.04189\n","\n","  Image Deraining via Self-supervised Reinforcement Learning\n","  https://arxiv.org/abs/2403.18270\n","\n","2. Deep learning implementations are always based on our observations to gradually improve the model. In the previous process, you might have noticed that although the deraining results seem quite good, the colors of the images may differ from the original ones. Please propose two ideas (no need to implement) on how we can improve the model based on the observation that **'the colors of the derained images differ from the input images.'** These ideas can be in any aspect such as the model, loss function, data preprocessing, data augmentation,or the whole training framework,etc.\n","\n","3. Many tasks in image processing and computer vision require separate models, each trained for a specific purpose, such as deraining, denoising, deblurring, and super-resolution. However, deploying multiple models in practical applications can be resource-intensive and inefficient. Please propose one idea on how we can develop an all-in-one model that can handle multiple image restoration tasks simultaneously.\n","\n","  You may find some interesting ideas in the following papers.\n","\n","  Restormer: Efficient Transformer for High-Resolution Image Restoration\n","  https://arxiv.org/abs/2111.09881\n","\n","  Language-driven All-in-one Adverse Weather Removal\n","  https://arxiv.org/abs/2312.01381"],"metadata":{"id":"fiUcWzhkwPFb"}}]}